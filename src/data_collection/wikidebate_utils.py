import json
import pandas as pd
import numpy as np
import glob
import ast
import mwparserfromhell
import requests
import re
import math
import os
from urllib.parse import urlparse, unquote
from time import sleep
#from revisions_utils import *

BASE_URL= "https://en.wikiversity.org/w/api.php"
API_URL = 'https://en.wikiversity.org/w/api.php'

LIST_OF_EXCEPTION = ['Are humans omnivores or herbivores?',
                     'Are medical doctors arrogant?',
                     'Is access date for online references and links worth providing?',
                     'Is bitcoin a good investment?',
                     'Is slavery good?',
                     'Is the output of ChatGPT copyrighted?',
                     'Should abortion be legal?',
                     'Should infanticide be legal?',
                     'Should non-practitioners avoid criticising practitioners?',
                     'Should Wikipedia administrators be required to cite offending diffs when they indefinitely block an account?',
                     'Should Wikipedia editors always provide an edit summary?',
                     'Should Wikiversity allow editors to post content generated by LLMs?',
                     'Should Wiktionary have topical categories?',
                     'Should Wiktionary votes cast be required to have a rationale?']


# Function to extract the title from the URL
def extract_title_from_url(url):
    path = urlparse(url).path
    title = path.split('/')[-1]
    return unquote(title)

# Function to fetch the content of a category page on Wikiversity
def fetch_wikiversity_category_content(title):
    
    params = {
        "action": "query",
        "list": "categorymembers",
        "cmtitle": title,
        "cmlimit": "max",  # Fetch as many members as possible (max is 500)
        "format": "json"
    }
    
    response = requests.get(BASE_URL, params=params)
    data = response.json()
    
    # Extracting category members
    members = data.get('query', {}).get('categorymembers', [])
    
    # Formatting the output
    content = "\n".join([member['title'] for member in members])
    
    return content

def get_all_wikidebate_titles():
    wiki_url= "https://en.wikiversity.org/wiki/Category:Wikidebates"
    title = extract_title_from_url(wiki_url)
    content = fetch_wikiversity_category_content(title)
    titles=[]
    title=" "
    for ch in content[41:]:
        if ch=='\n':
            titles.append(title.lstrip())
            title=" "
            continue
        title+=ch
    return titles

def return_page_rev_dict(rev_data,keys,page_revision_dict):
    for key in keys:
        if key == 'content':
            if '*' not in rev_data['slots']['main']:
                print("no conetnt found for this revision")
                page_revision_dict[key].append('')
            else:
                page_revision_dict[key].append(rev_data['slots']['main']['*'])
        else:
            if key not in rev_data:
                page_revision_dict[key].append(0)
            elif key=='minor' or key=='anon':
                page_revision_dict[key].append(1)
            else:
                page_revision_dict[key].append(rev_data[key])
    return page_revision_dict

def collect_revisions_full_data(titles):
    users_dict={}
    users_page_id_dict={}
    users_m_flags_dict={}
    users_anon_flag={}
    users_page_id_dict_repeted={}
    keys=['revid','parentid','user','timestamp','size','content','comment','minor','tags','anon']
    path = 'data/argumentation/revisions_data/'
    for title in titles:
        if '/' in title:
            print("found unacceptable characther in title")
            file_name=title.replace('/','_')
        else:
            file_name=title
        _,_,_,_,page_dict = request_page_revisions_infos(title,users_dict,users_page_id_dict,users_m_flags_dict,users_anon_flag,users_page_id_dict_repeted)
        test_len= len(page_dict['revid'])
        for key in keys:
            if len(page_dict[key])!=test_len:
                print(f"error in {title}")
                continue
        page_df = pd.DataFrame.from_dict(page_dict)
        page_csv = page_df.to_csv(path+file_name+'.csv')
    return page_csv,file_name

def request_page_revisions_infos(TITLE,users,users_page_id,users_m_flags,users_anon_flag, user_page_id_repeted,LANG='en'):
    global N_RESPONSE_EXCEPTIONS
    
    session = requests.Session()

        
    PARAMS = {
        "action": "query",
        "prop": "revisions",
        "format": "json",
        "titles": TITLE,
        "rvprop": "ids|timestamp|user|userid|content|flags|tags|size|contentmodel|comment",
        "rvslots": "main",
        "rvdir": "newer", 
        "rvlimit":"max",
        # "formatversion": "2",
        "continue": ""

    }
    
    N_RV=0
    N_users=0
    hiddenusers = 0
    N_minor=0
    page_users={}
    keys=['revid','parentid','user','timestamp','size','content','comment','minor','tags','anon']
    page_revisions_dict = {
        'revid':[],
        'parentid':[],
        'user':[],
        'timestamp':[],
        'size':[],
        'content':[],
        'comment':[],
        'minor':[],
        'tags':[],
        'anon':[],
    }

    while True:
        
        while True:
            _exception_counter=0
            try:
                _response = session.get(url=API_URL, params=PARAMS, timeout=60*10)
                break
            except:
                _exception_counter+=1
                N_RESPONSE_EXCEPTIONS+=1
                if _exception_counter==10:
                    return 0
                sleep(10)
        
        response_data = _response.json()
        response_pages = response_data["query"]["pages"]
        
        for page_id, page_data in response_pages.items():
            if not 'revisions' in page_data:
                print(f'revisions not found in response data for page {TITLE}({LANG}); response data:\n\n')
                print(response_data)
                return 0
            revisions = page_data['revisions']         
            N_RV+=len(revisions)
            print(f"page {TITLE} found {N_RV} revisions", end=' '*15+'\r')
            for rev_data in revisions:
                if not 'user' in rev_data:
                    hiddenusers+=1
                    continue
                if rev_data["user"] not in page_users:
                    #users[rev_data["user"]]= 1
                    page_users[rev_data["user"]]=1
                    if rev_data["user"] in users:
                        users[rev_data["user"]] += 1
                        users_page_id[rev_data["user"]].append(int(page_id))
                        user_page_id_repeted[rev_data['user']].append(int(page_id))
                    else:
                        users[rev_data["user"]] = 1
                        users_page_id[rev_data["user"]] = [int(page_id)]
                        user_page_id_repeted[rev_data['user']] = [int(page_id)]
                    N_users+=1
                else:
                    users[rev_data["user"]] += 1
                    user_page_id_repeted[rev_data['user']].append((int(page_id)))
                if 'minor' in rev_data:
                    N_minor+=1
                    if rev_data['user'] in users_m_flags:
                        users_m_flags[rev_data['user']]+=1
                    else: 
                        users_m_flags[rev_data['user']]=1
                if rev_data['user'] not in users_anon_flag:
                    if 'anon' in rev_data:
                        users_anon_flag[rev_data['user']]=True
                    else:
                        users_anon_flag[rev_data['user']]=False
                
                page_revisions_dict = return_page_rev_dict(rev_data,keys,page_revisions_dict)

        if 'continue' in response_data:
            PARAMS['continue'] = response_data['continue']['continue']
            PARAMS['rvcontinue'] = response_data['continue']['rvcontinue']
        
        else:
            return N_RV, N_users, N_minor, hiddenusers, page_revisions_dict

def get_all_wikidebates_revisions_infos(titles):
    num_revisions=[]
    num_users=[]
    num_minor_revisions=[]
    num_hiddenusers=[]
    users_dict={}
    users_page_id_dict={}
    users_m_flags_dict={}
    users_anon_flag={}
    users_page_id_dict_repeted={}
    for title in titles:
        N_RV, N_users, N_minor, hiddenusers = request_page_revisions_infos(title,users_dict,users_page_id_dict,users_m_flags_dict,users_anon_flag, users_page_id_dict_repeted)
        num_revisions.append(N_RV)
        num_users.append(N_users)
        num_minor_revisions.append(N_minor)
        num_hiddenusers.append(hiddenusers)
    return num_revisions,num_users,num_minor_revisions,num_hiddenusers,users_dict,users_page_id_dict,users_m_flags_dict,users_anon_flag,users_page_id_dict_repeted

def parse(title):
    params = {
        "action": "query",
        "prop": "revisions|info",
        "inprop":"url",
        "rvprop": "content",
        "rvslots": "main",
        "rvlimit": 1,
        "titles": title,
        "format": "json",
        "formatversion": "2",
    }
    headers = {"User-Agent": "My-Bot-Name/1.0"}
    req = requests.get(BASE_URL, headers=headers, params=params)
    res = req.json()
    revision = res["query"]["pages"][0]
    text = revision['revisions'][0]['slots']['main']['content']
    link = revision["fullurl"]
    return mwparserfromhell.parse(text),link

def get_items(code):
    sentences=[]
    for section in code.get_sections(levels=[3],flat=True):
        count=0
        item=" "
        for ch in section.strip_code():
            if ch=='\n':
                if count>0:
                    if item.lstrip().startswith("Category:"):
                        item=''
                        continue
                    if len(item)>3:
                        sentences.append(item.lstrip())
                    item=" "
                    continue
                else:
                    item=" "
                    count+=1
                    continue
            item+=ch
        if item.lstrip().startswith("Category:"):
            item=''
            continue
        sentences.append(item.lstrip())
    return sentences

def get_reference_infos(code):
    references=[]
    refered_sentences=[]
    ref_counter=0
    last_setnece=''
    for section in code.get_sections(levels=[3],flat=True):
        tags = code.filter_tags()
        prev_node = section.nodes[0]
        for node in section.nodes:
            if '<ref' in node and node in tags:
                ref_counter +=1
                if node not in references:
                    references.append(str(node))
                if prev_node not in references and '<ref name' not in prev_node:
                    refered_sentences.append(str(prev_node))
                    last_setnece=prev_node
                else:
                    refered_sentences.append(last_setnece)
            elif '<ref name' in node and node in tags :
                if prev_node not in references  and '<ref name' not in prev_node:
                    refered_sentences.append(str(prev_node))
                    last_setnece=prev_node
                else:
                    refered_sentences.append(last_setnece)
            if len(str(node))>5 and node not in code.filter_templates():
                prev_node=node
    return [refered_sentences,len(refered_sentences),ref_counter,len(references),references]


def collect_references_wikidebate(items,refered_sentences):
    xl_references =[]
    for item in items:
        item_ref = 0
        for i,sent in enumerate(refered_sentences):
            if sent.lstrip() in item:
                item_ref+=1
        xl_references.append(item_ref)
    return xl_references


def get_all_wikidebate_content(titles):
    id=1000
    all_items=[]
    unique_page_ids=[]
    page_ids=[]
    dict_titles=[]
    xl_references=[]
    links=[]
    obj_tot=[]
    levels_tot=[]
    sections_flag = ["pro","con"]
    for title in titles:
        print(f"====== new debate: {title} ======")
        obj_page=[]
        levels_page=[]
        code,link=parse(title)
        refered_sentences,_,_,_,references = get_reference_infos(code)
        sections = code.get_sections(levels=[3],flat=True)
        items=get_items(code)
        if title in LIST_OF_EXCEPTION:
            items = manage_exception(items,title,LIST_OF_EXCEPTION)
        if len(references)>0:
            xl_references += collect_references_wikidebate(items,refered_sentences)
        else:
            xl_references += [0]*len(items)
        for i,section in enumerate(sections):
            objection_flags,levels = get_arguemnt_type_labels(section,items,section_label=sections_flag[i%2])
            obj_page+=objection_flags
            levels_page+=levels
        if len(items)!=len(obj_page):
            print(f"{title} has an issue with the number of items")
        unique_page_ids.append(id)
        page_ids+=[unique_page_ids[-1]]*len(items)
        dict_titles+=[title]*len(items)
        all_items+=items
        obj_tot+=obj_page
        levels_tot+=levels_page
        id+=1
        links+=[link]*len(items)
        string_type_of_arg=get_string_type(obj_tot)
    content_dict = {
        'page_id':page_ids,
        'link':links,
        'title':dict_titles,
        'references':xl_references,
        'text':all_items,
        'type':string_type_of_arg,
        'level':levels_tot,
    }
    content_df=pd.DataFrame(content_dict)
    return content_df


def update_all_items(version):
    titles = get_all_wikidebate_titles()
    content_df = get_all_wikidebate_content(titles)
    content_df.to_csv(f'data/global/all_items_v{version}.csv')
    return content_df


def get_arguemnt_type_labels(section,items,section_label):
   
    j=0
    levels=[]
    objection_flags=[]
    objection_label=0
    count=0
    templates=['{{argument for}}','{{argument against}}','{{objection}}','{{comment}}' ]
    args_and_objs=[node.lower() for node in section.filter_templates() if node.lower() in templates]
    for node in section.nodes:
        # print(node)
        # print(f"j is :{j}")
        # print(node)
        if node == '\n' or node == '\n\n':
            continue
        if node == '*':
            count+=1
            continue
        if node.lower() in args_and_objs or node.lstrip().lower().startswith('comment') or node=="''Comment''":
            if node.lower()=='{{objection}}': 
                #print("recognized objection")
                objection_label=1
            if node.lower()=='{{comment}}' or node.lstrip().lower().startswith('comment') or node=="''Comment''":
                objection_label=2
            if node.lower()=='{{argument for}}' or node.lower()=='{{argument against}}':
                #print("recognized argument")
                objection_label=0
            
            
            levels.append(count)
            objection_flags.append(objection_label)
            #print(f"{item[:30]} recognized as {objection_label}")
            objection_label=0
            count=0
            j+=1
        if j>=len(items):
            break
    return objection_flags,levels


def manage_exception(items,title, LIST_OF_EXCEPTION):
    if title == LIST_OF_EXCEPTION[1]:
        items[9:11]=[''.join(items[9:11])]
    if title == LIST_OF_EXCEPTION[5]:
        items[0:5]=[''.join(items[0:5])]
    if title == LIST_OF_EXCEPTION[9]:
        items.pop(-1)
    if title == LIST_OF_EXCEPTION[13]:
        items.pop(-1)
    return items

def get_string_type(obj_tot):
    type_of_item=[]
    for elem in obj_tot:
        if elem==0:
            type_of_item.append("Argument")
        if elem==1:
            type_of_item.append("objection")
        if elem==2:
            type_of_item.append("Comment")
    return type_of_item

def build_modfications_dict(titles):
    date_zero='2018-08-01T00:00:00Z'
    votes_start = '2018-08-01T00:00:00Z'
    votes_end = '2024-10-01T00:00:00Z'
    votes_dates = convert_timestamps([votes_start,votes_end])
    keys = ['revid','parentid','itemsids','addedids','modifiedids','delatedids','items','moditems','delateditems','timedist','author','author_diff','minor','timestamp']
    survivors_dict = {}
    for key in keys:
        survivors_dict[key]=[]

    for num_page,title in enumerate(titles):
        count_skipped_rev=0
        survivors_dict = {}
        for key in keys:
            survivors_dict[key]=[]
        print(f" =========== new debate : {title} =============")
        page_id = 1000+num_page
        page_id = str(page_id)
        first_rev = True
        count_minor_rev=0
        prev_size=0
        page_revisions_data = pd.read_csv(f'data/argumentation/revisions_data/{title}.csv',index_col=0)
        page_revisions_data = page_revisions_data[page_revisions_data['content'].notna()].reset_index(drop=True)
        for k,data in page_revisions_data.iterrows():
            print(f"revision number {k}")
            
            #Initialize necessary arrays
            curr_items_ids = []       
            modids = []
            deletedids=[]
            moditems=[]
            deleteditems=[]
            curr_items=[]
            curr_timestamp = data['timestamp']
            minor_flag=0
            
            compare_timestamps = convert_timestamps([curr_timestamp,date_zero])
            if compare_timestamps[0]<compare_timestamps[1]:
                prev_size=data['size']
                prev_user = data['user']
                print("before date zero")
                continue
            
            curr_revid = data['revid']
            curr_parrentid = data['parentid']
            curr_user = data['user']
            
            
            curr_size = data['size']
            curr_content = data['content']
            curr_comment = data['comment']
            curr_minor_flag = data['minor']
            curr_tags = data['tags']
            curr_anon_flag = data['anon']
            size_diff=(curr_size-prev_size)
            curr_code = mwparserfromhell.parse(curr_content)
            print(curr_timestamp)
            print(f"size: {curr_size}")

            #Check votes presence
            if votes_dates[0]<compare_timestamps[0]<votes_dates[1]:
                curr_code = manage_votes_presence(curr_code)
            
            #Detect vandalism
            if first_rev==False and prev_size!=0 and size_diff/prev_size<=-0.5:
                print("DETECTED POSSIBLE VANDALISM: skip this revision")
                count_skipped_rev+=1
                prev_size=curr_size
                prev_timestamp = curr_timestamp
                prev_items = survivors_dict['items'][-1]
                continue

            #Manage minor revisons
            if curr_minor_flag==1:
                if first_rev==True:
                    print("first rev is a minor rev")
                    continue 
                #prev_items = []
                print("=============MINOR REVISION============")
                count_minor_rev+=1
                minor_flag=1
                #prev_size=curr_size
                #prev_timestamp = curr_timestamp
                
                #curr_sections,prev_items = get_sections_items(curr_code,title,curr_timestamp)
            
                #For those cases where the strucutre of the debate is changed in a minor revision
                # if len(curr_sections)!=len(survivors_dict['itemsids'][-1]):
                #     prev_items = survivors_dict['items'][-1]
                #     print("changed structure: ignore revision")
        
                #continue

            # Manage first revision
            if first_rev==True or len(prev_items)==0:
                
                #Get current sections and current items
                curr_sections,curr_items = get_sections_items(curr_code,title,curr_timestamp)
                #Check if there are any items
                if len(curr_sections)>0:
                    total_n_items=0
                    for sec_items in curr_items:
                        total_n_items+=len(sec_items)
                if total_n_items==0:
                    print("ERROR: NO ITEMS FOUND")

                #Collect all the ids from the sections
                for i,sec in enumerate(curr_items):
                    secid=i
                    sec_ids=[]
                    for j,elem in enumerate(sec):
                        num_comment = str(j).zfill(4)
                        sec_ids.append(page_id+'_'+f'{k}'+'_'+f'{secid}'+'_'+f'{num_comment}')
                    curr_items_ids.append(sec_ids)

                added_ids = [item for sublist in curr_items_ids for item in sublist]
            
                cleaned_timestamps = convert_timestamps([curr_timestamp,curr_timestamp])
                timedist=cleaned_timestamps[1]-cleaned_timestamps[0]
                user_diff=0
                if first_rev==False:
                    if prev_user==curr_user:
                        user_diff=1
                first_rev=False

                curr_dict_items = [curr_revid,curr_parrentid,curr_items_ids,added_ids,modids,deletedids,curr_items,moditems,deleteditems,timedist,curr_user,user_diff,minor_flag,curr_timestamp]
                for (key,item) in zip(keys,curr_dict_items):
                    survivors_dict[key].append(item)


            #Manage other revisions
            else:
                #Initialize addedids array to collect ids of added items
                added_ids=[]
                ##Check if user changed
                user_diff=0
                if curr_user==prev_user:
                    user_diff=1
                #Calculate timedistance
                cleaned_timestamps = convert_timestamps([prev_timestamp,curr_timestamp])
                timedist = cleaned_timestamps[1]-cleaned_timestamps[0]

                #Get current sections and items
                curr_sections,curr_items = get_sections_items(curr_code,title,curr_timestamp)
    
                #Check if there are any items
                if len(curr_sections)>0:
                    total_n_items=0
                    for sec_items in curr_items:
                        total_n_items+=len(sec_items)
                if total_n_items==0:
                    print("ERROR: NO ITEMS FOUND")

                #If the number of sections is mantained equal to the previous revision's
                if len(prev_items)==len(curr_sections):
                    for i,(curr_item,prev_item) in enumerate(zip(curr_items,prev_items)):
                        prev_items_ids=survivors_dict['itemsids'][-1][i]
                        curr_sec_items = curr_item
                        modifications = compare_lists(prev_item,curr_sec_items)
                        #CHIAMATA ALLA FUNZIONE PER COLLEZIONARE GLI IDS
                        new_items_id_sec, added_ids_sec, deleted_ids_sec, modified_ids_sec = obtain_correct_ids(curr_sec_items,prev_item,modifications,prev_items_ids,page_id,k,i)
                        #print(f"{count_added}, count mod: {count_modifications}, count_removed: {count_removed}")
                        added_ids+=added_ids_sec
                        deletedids+=deleted_ids_sec
                        modids += modified_ids_sec
                        curr_items_ids.append(new_items_id_sec)
                        moditems += modifications['modified']
                        deleteditems += modifications['deleted']

                #If the number of sections is different between revisions
                if len(prev_items)!=len(curr_sections):
                    print("number of sections changed")
                    prev_items_ids = survivors_dict['itemsids'][-1]
                    curr_items_ids,curr_items,added_ids,deletedids,modids,modifications = manage_different_section_number(prev_items,prev_items_ids,curr_items,page_id,k)
                    
                curr_dict_items = [curr_revid,curr_parrentid,curr_items_ids,added_ids,modids,deletedids,curr_items,moditems,deleteditems,timedist,curr_user,user_diff,minor_flag,curr_timestamp]
                for (key,item) in zip(keys,curr_dict_items):
                    survivors_dict[key].append(item)
            
            #Sanity check
            print(len(curr_sections))
            print("items lenght")
            for con,sec_items in enumerate(curr_items):
                print(f"section {con} items: {len(sec_items)}")
            if len(curr_items)!=len(curr_sections):
                print("WARNING: number of sections differs from number of items sections")
            if len(curr_items_ids)!=len(curr_sections):
                print("WARNING: number of sections differs from number of ids sections")
            if len(curr_items_ids)!=len(curr_items):
                print("WARNING: number of ids sections differs from number of items sections")
            for l,(sec_it, sec_id) in enumerate(zip(curr_items,curr_items_ids)):
                if len(sec_it)!=len(sec_id):
                    print(f"WARNING: num of elements in section {l} doesn't match")

            prev_size=curr_size
            prev_code = curr_code
            prev_items = curr_items
            prev_user = curr_user
            prev_timestamp = curr_timestamp
        
        json_path=f'data/argumentation/survivors_data/{title}.json'
        with open(json_path, 'w') as file:
            json.dump(survivors_dict, file, indent=4, default=str)
            file.write('\n')
    
    return 

def create_survivors_df(flat_ids,flat_items,flat_num_rev_survived,num_modifications,mod_ratio,num_diff_contributors,authors):
    survivors_dict = {
        'id':flat_ids,
        'item':flat_items,
        'num rev survived': flat_num_rev_survived,
        'num modifications':num_modifications,
        'avg mod ratio': mod_ratio,
        'num unique contributors': num_diff_contributors,
        'authors': authors
    }
    survivors_df = pd.DataFrame(survivors_dict,dtype=object)
    return survivors_df


def get_number_and_avg_ratio_of_modifications(data):
    num_modifications = []
    mod_ratio = []
    num_contributors = []
    diff_authors=[]
    
    flat_ids = [id for sublist in data['itemsids'][-1] for id in sublist]
    for id in flat_ids:
        find_creator = True
        authors = []
        num_contrib_item = 1
        count_mod=0
        mod_ratio_value=0
        mod_ratio_count=0
        
        for (rev,mod,author,add) in zip(data['modifiedids'],data['moditems'],data['author'],data['addedids']):
            if len(rev)!=len(mod):
                continue
            if find_creator==True:
                if id in add:
                    authors.append(author)
            if id in rev:
                index = rev.index(id)
                count_mod+=1
                mod_ratio_count+=1
                mod_ratio_value+=mod[index][2]
                if author not in authors:
                    num_contrib_item+=1
                    authors.append(author)
        num_contributors.append(num_contrib_item)
        diff_authors.append(authors)
        if mod_ratio_count>0:
            mod_ratio.append(mod_ratio_value/mod_ratio_count)
        else:
            mod_ratio.append(0.0)
        num_modifications.append(count_mod)
    return num_modifications,mod_ratio,num_contributors,diff_authors

def extract_rev_number(items_ids):
    first_rev_number = []
    for rev in items_ids:
        sec_list=[]
        for sect in rev:
            items_list=[]
            for item in sect:
                rev_num = item[5:]
                index_ = rev_num.find('_')
                rev_num=int(rev_num[:index_])
                items_list.append(rev_num)
            sec_list.append(items_list)
        first_rev_number.append(sec_list)
    return first_rev_number


def build_csv_mod(dir_files):
    percentage_survived=[]
    number_survived = []
    number_revision = []
    number_modifications=[]
    ratio_modifications = []
    percentage_of_relevant_modifications = []
    perc_of_item_modified = []
    titles=[]
    number_of_different_authors=[]

    for dir_file in dir_files:
        with open(dir_file, 'r') as file:
            data = json.load(file)
            titles.append(dir_file.replace('data/argumentation/survivors_data/','').replace('.json',''))
        
        print(f"new debate: {titles[-1]}")
        num_considered_rev=len(data['revid'])
        rev_df = pd.read_csv('data/argumentation/revisions_data/'+titles[-1]+'.csv',index_col=0)
        df_from_first_rev = rev_df[rev_df['revid']>data['revid'][0]]
        num_skipped_rev = len(rev_df)-len(df_from_first_rev)
        first_rev = df_from_first_rev[0:1]
        starting_timestamp=first_rev['timestamp']
        first_rev = extract_rev_number(data['itemsids'])
        num_survived_rev=[]
        print(num_considered_rev)
        for i in range(len(first_rev[-1])):
            num_survived_rev.append([len(rev_df)-x for x in first_rev[-1][i]])
        
        flat_num_rev_survived = [num for sublist in num_survived_rev for num in sublist]
        flat_items = [item for sublist in data['items'][-1] for item in sublist]
        flat_ids = [id for sublist in data['itemsids'][-1] for id in sublist]
        
        number_of_different_authors.append(len(list(set(data['author']))))
        num_modifications,mod_ratio,num_diff_contributors,authors = get_number_and_avg_ratio_of_modifications(data)
        print((num_diff_contributors))
        data_survivors_df = create_survivors_df(flat_ids,flat_items,flat_num_rev_survived,num_modifications,mod_ratio,num_diff_contributors,authors)
        data_survivors_csv = data_survivors_df.to_csv(f'data/argumentation/items survivors infos/{titles[-1]}.csv')
        avg_rev_survived = data_survivors_df['num rev survived'].mean()
        above_zero_ratio_df = data_survivors_df[data_survivors_df['avg mod ratio']>0.0]
        avg_ratio = above_zero_ratio_df['avg mod ratio'].mean()
        num_modified_item = len(above_zero_ratio_df)
        if len(above_zero_ratio_df)==0:
            avg_ratio=1.00000
        number_revision.append(num_considered_rev)
        number_survived.append(avg_rev_survived)
        number_modifications.append(np.mean(num_modifications))
        ratio_modifications.append(np.mean(avg_ratio))
        perc_of_item_modified.append(num_modified_item/len(data_survivors_df))
        percentage_of_relevant_modifications.append(len(data_survivors_df[data_survivors_df['avg mod ratio'].between(0.1,0.85)])/len(data_survivors_df))
        percentage_survived.append(avg_rev_survived/len(data['revid']))

    return 


def updating_routine(title):
    dir_files=[]
    if title[0] == 'Was 9_11 an inside job?':
        title[0]=title[0].replace('_','/')
    csv,title_new = collect_revisions_full_data(title)
    if title[0] == 'Was 9/11 an inside job?':
        title[0]=title_new
    build_modfications_dict(title)
    for tit in title:
        dir_files.append('data/argumentation/survivors_data/'+tit+'.json')
    build_csv_mod(dir_files=dir_files)


