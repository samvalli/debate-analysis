{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6ef96d",
   "metadata": {},
   "source": [
    "# Debate analysis example\n",
    "\n",
    "In this notebook we present an example of how to evaluate the aspects we defined on shprt debate. We will use as example a the topic of Universal Basic Income. \n",
    "We will use three different debates: \n",
    "* one from [Wikidebate](https://en.wikiversity.org/wiki/Category:Wikidebates) titled ['_Should universal basic income be established?_'](https://en.wikiversity.org/wiki/Should_universal_basic_income_be_established%3F)\n",
    "* one from [Kialo](https://www.kialo.com) titled ['_Should governments provide a universal basic income?_'](https://www.kialo.com/should-governments-provide-a-universal-basic-income-14053)\n",
    "* one from [/rchangemyview](https://www.reddit.com/r/changemyview/) titled ['_CMV: Universal basic income is the way of the future._'](https://www.reddit.com/r/changemyview/comments/tdmuae/cmv_universal_basic_income_is_the_way_of_the/).\n",
    "\n",
    "For further general information on data collection see the readme file and `src/data_collection/` folder for the code.\n",
    "\n",
    "We start by importing all the functions we will use to evaluate the defined metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.complexity_utils import *\n",
    "from src.disagreement_utils import *\n",
    "from src.equality_engagement_utils import *\n",
    "from src.reason_utils import *\n",
    "from src.sentiment_utils import *\n",
    "from src.sourcing_utils import *\n",
    "from src.topic_distance_utils import *\n",
    "\n",
    "from sentence_transformers import SentenceTransformer,util,SimilarityFunction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f6ec1",
   "metadata": {},
   "source": [
    "We proceeed to import the data we need. In this notebook we have three separated csv files, each row of these files contain a post, which is assigned with the following infomration:\n",
    "* **id**: a unique id identifying the post\n",
    "* **page_id**: an id identifying the topic \n",
    "* **item**: the content of the post\n",
    "* **parent_id**: the id of the parent post, in the case of root post this value is 0\n",
    "* **title**: a title identifying the topic\n",
    "* **debate_id**: an id identifying the debate (as Kialo and CMV in the original dataset may have different debates for the same topic)\n",
    "* **length**: post length in charachters\n",
    "* **level**: depth level of the post (root posts are assigned with a level of 0)\n",
    "* **thread_id**: an id identifying the thread each post belongs to\n",
    "* **author**: author(s) of the post, in the case of Wikidebate more than one user could be involved. In the case of Kialo we cannot associate each post with its authors, thus this column is assigned with Nan values, while we use unmatched statistics when needed.\n",
    "* **platform**: short name of the platform the post was published in. \n",
    "\n",
    "Moreover Canghe My View has:\n",
    "\n",
    "* **original_item**: post content before preprocessing, used to extract references. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "platforms=['wiki','kialo','cmv']\n",
    "wiki_data=pd.read_csv('data/UBI_wiki.csv',index_col=0)\n",
    "kialo_data=pd.read_csv('data/UBI_kialo.csv',index_col=0)\n",
    "cmv_data=pd.read_csv('data/UBI_cmv.csv',index_col=0)\n",
    "merged_data = pd.concat([wiki_data,kialo_data,cmv_data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1c17",
   "metadata": {},
   "source": [
    "### Engagement & Equality\n",
    "We now evaluate the engagement and the equality score for our data. The function provide us with two lists: the first one contains engagement scores for respectively Wikidebate, Kialo and CMV, while the second equality scores in the same order of appereance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495901c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_value,equality_value=engagement_and_equality_assignment(merged_data,platforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0694ad",
   "metadata": {},
   "source": [
    "### Sourcing \n",
    "To evaluate the sourcing score we first get the number of reference contained in each debate using the function *get_platforms_reference_number*, which provide a list containing Wikidebate, Kialo and CMV debates total number of references. \n",
    "Then we evaluate the actual score by dividing this number by the number of total posts contained in the debate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_references=get_platforms_reference_number(merged_data,platforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aacdd0",
   "metadata": {},
   "source": [
    "### Topic diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7b3f6",
   "metadata": {},
   "source": [
    "In order to evaluate the topic diversity we employ the `SentenceTransformers` and the `transformers` libraries. In particoular we usde the `all-MiniLM-L6-v2` model. The first step is thus to import the model and the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4835f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sbert_name = \"all-MiniLM-L6-v2\"\n",
    "model_sbert = SentenceTransformer(model_sbert_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b48645",
   "metadata": {},
   "source": [
    "First we collect the threads ids in a matrix (`threads_id_matrices`) that we will use to keep track of the thread each post belongs to. We will use this matrix later.\n",
    "Then we obtain matrices containing the embeddings of each post (`embeddings_matrices`), these embeddings are obtained by using the model we imported. The `debate_matrices` contain the `debate_id` of each post and are useful to keep track of the debate each post belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628abce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_id_matrices=get_thread_id_claims(merged_data,platforms)\n",
    "embeddings_matrices, debate_matrices, documents_matrices = get_sentence_transformers_claim_embeddings(merged_data, platforms, model_sbert, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49f9d6",
   "metadata": {},
   "source": [
    "The embeddings of each post belonging to the same thread are aggregrated by averaging on them. The distance between threads of the same debate is then obtained as 1-cosine similarity. \n",
    "It is recomanded to set a `treshold` thant controls the maximum number of posts to consider for each thread. In this case a number equal to `treshold` of random posts is selected, and this operation is repeted `num_runs` times, to insure the reliability.\n",
    "The function `obtain_debate_wise_topic_distance` returns a vector containing the average of topic distances between threads of each debate.\n",
    "In our case in, in fact, a list containing respectevely the Wikidebate, Kialo and CMV debate's topic distance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = 'data/st_embeddings'\n",
    "deb_dir = 'data/st_debates'\n",
    "treshold=11\n",
    "\n",
    "debate_wise_topic_distance = obtain_debate_wise_topic_distance(merged_data,platforms,treshold,emb_dir,deb_dir,\n",
    "                                                            threads_id_matrices,model_sbert,num_runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78201ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity=VADER_sentiment(merged_data)\n",
    "sentiment=tex_blob_sentiment(merged_data,platforms)\n",
    "mltd=mltd(merged_data)\n",
    "readability_score=readability_score(merged_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
