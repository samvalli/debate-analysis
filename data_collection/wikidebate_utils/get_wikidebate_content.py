import requests
import mwparserfromhell
import pandas as pd
import re
from global_utils.get_argument_type import get_arguemnt_type_labels,manage_exception,get_string_type
from global_utils.get_wikidebate_titles import get_all_wikidebate_titles

BASE_URL= "https://en.wikiversity.org/w/api.php"

LIST_OF_EXCEPTION = ['Are humans omnivores or herbivores?',
                     'Are medical doctors arrogant?',
                     'Is access date for online references and links worth providing?',
                     'Is bitcoin a good investment?',
                     'Is slavery good?',
                     'Is the output of ChatGPT copyrighted?',
                     'Should abortion be legal?',
                     'Should infanticide be legal?',
                     'Should non-practitioners avoid criticising practitioners?',
                     'Should Wikipedia administrators be required to cite offending diffs when they indefinitely block an account?',
                     'Should Wikipedia editors always provide an edit summary?',
                     'Should Wikiversity allow editors to post content generated by LLMs?',
                     'Should Wiktionary have topical categories?',
                     'Should Wiktionary votes cast be required to have a rationale?']

def parse(title):
    params = {
        "action": "query",
        "prop": "revisions|info",
        "inprop":"url",
        "rvprop": "content",
        "rvslots": "main",
        "rvlimit": 1,
        "titles": title,
        "format": "json",
        "formatversion": "2",
    }
    headers = {"User-Agent": "My-Bot-Name/1.0"}
    req = requests.get(BASE_URL, headers=headers, params=params)
    res = req.json()
    revision = res["query"]["pages"][0]
    text = revision['revisions'][0]['slots']['main']['content']
    link = revision["fullurl"]
    return mwparserfromhell.parse(text),link

def get_items(code):
    sentences=[]
    for section in code.get_sections(levels=[3],flat=True):
        count=0
        item=" "
        for ch in section.strip_code():
            if ch=='\n':
                if count>0:
                    if item.lstrip().startswith("Category:"):
                        item=''
                        continue
                    if len(item)>3:
                        sentences.append(item.lstrip())
                    item=" "
                    continue
                else:
                    item=" "
                    count+=1
                    continue
            item+=ch
        if item.lstrip().startswith("Category:"):
            item=''
            continue
        sentences.append(item.lstrip())
    return sentences

def get_reference_infos(code):
    references=[]
    refered_sentences=[]
    ref_counter=0
    last_setnece=''
    for section in code.get_sections(levels=[3],flat=True):
        tags = code.filter_tags()
        prev_node = section.nodes[0]
        for node in section.nodes:
            if '<ref' in node and node in tags:
                ref_counter +=1
                if node not in references:
                    references.append(str(node))
                if prev_node not in references and '<ref name' not in prev_node:
                    refered_sentences.append(str(prev_node))
                    last_setnece=prev_node
                else:
                    refered_sentences.append(last_setnece)
            elif '<ref name' in node and node in tags :
                if prev_node not in references  and '<ref name' not in prev_node:
                    refered_sentences.append(str(prev_node))
                    last_setnece=prev_node
                else:
                    refered_sentences.append(last_setnece)
            if len(str(node))>5 and node not in code.filter_templates():
                prev_node=node
    return [refered_sentences,len(refered_sentences),ref_counter,len(references),references]

def get_reference_infos_unused(code):
    references=[]
    refered_sentences=[]
    ref_counter=0
    for section in code.get_sections(levels=[3],flat=True):
        tags = code.filter_tags()
        prev_node = section.nodes[0]
        for node in section.nodes:
            print(node)
            if '</ref>' in node and node in tags:
                ref_counter +=1
                print(ref_counter)
                references.append(str(node))
                if prev_node not in references and '<ref name' not in prev_node:
                    refered_sentences.append(str(prev_node))
            elif '<ref name' in node and node in tags :
                if prev_node not in references  and '<ref name' not in prev_node:
                    refered_sentences.append(str(prev_node))
            if len(str(node))>5 and node not in code.filter_templates():
                prev_node=node
    return [refered_sentences,len(refered_sentences),ref_counter,references]

def collect_references_wikidebate_unused(items,refered_sentences,references):
    xl_references =[]
    for item in items:
        string_ref=''
        for i,sent in enumerate(refered_sentences):
            if sent.lstrip() in item:
                string_ref+=references[i]
        string_ref = re.findall(r'(https?://\S+)',string_ref)
        xl_references.append(string_ref)
    return xl_references

def collect_references_wikidebate(items,refered_sentences):
    xl_references =[]
    for item in items:
        item_ref = 0
        for i,sent in enumerate(refered_sentences):
            if sent.lstrip() in item:
                item_ref+=1
        xl_references.append(item_ref)
    return xl_references


def get_all_wikidebate_content(titles):
    id=1000
    all_items=[]
    unique_page_ids=[]
    page_ids=[]
    dict_titles=[]
    xl_references=[]
    links=[]
    obj_tot=[]
    levels_tot=[]
    sections_flag = ["pro","con"]
    for title in titles:
        print(f"====== new debate: {title} ======")
        obj_page=[]
        levels_page=[]
        code,link=parse(title)
        refered_sentences,_,_,_,references = get_reference_infos(code)
        sections = code.get_sections(levels=[3],flat=True)
        items=get_items(code)
        if title in LIST_OF_EXCEPTION:
            items = manage_exception(items,title,LIST_OF_EXCEPTION)
        if len(references)>0:
            xl_references += collect_references_wikidebate(items,refered_sentences)
        else:
            xl_references += [0]*len(items)
        for i,section in enumerate(sections):
            objection_flags,levels = get_arguemnt_type_labels(section,items,section_label=sections_flag[i%2])
            obj_page+=objection_flags
            levels_page+=levels
        if len(items)!=len(obj_page):
            print(f"{title} has an issue with the number of items")
        unique_page_ids.append(id)
        page_ids+=[unique_page_ids[-1]]*len(items)
        dict_titles+=[title]*len(items)
        all_items+=items
        obj_tot+=obj_page
        levels_tot+=levels_page
        id+=1
        links+=[link]*len(items)
        string_type_of_arg=get_string_type(obj_tot)
    content_dict = {
        'page_id':page_ids,
        'link':links,
        'title':dict_titles,
        'references':xl_references,
        'text':all_items,
        'type':string_type_of_arg,
        'level':levels_tot,
    }
    content_df=pd.DataFrame(content_dict)
    return content_df

def get_wikidebate_category(titles):
    categories=[]
    for title in titles:
        debate_categories=[]
        flag_cat=0
        code = parse(title)
        for node in code.nodes:
            if '[[Category:' in node:
                flag_cat=1
                category=str(node).replace('[[',' ').replace('Category:',' ').replace(']]',' ').lstrip().rstrip()
                debate_categories.append(category)
        if flag_cat==0:
            print("no category found")
        categories.append(debate_categories)
    return categories

def update_all_items(version):
    titles = get_all_wikidebate_titles()
    content_df = get_all_wikidebate_content(titles)
    content_df.to_csv(f'data/global/all_items_v{version}.csv')
    return content_df
